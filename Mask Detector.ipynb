{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "#importing dependencies related to nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#importing dependencies related to image transformations\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "#importing dependencies related to data loading\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#importing Tensorboard for data visualization\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/suhruth/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'shufflenet_v2_x1_0', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'WithMask': 0, 'WithoutMask': 1}\n",
      "['WithMask', 'WithoutMask']\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "import cv2,os\n",
    "\n",
    "data_path='/home/suhruth/Downloads/archive(1)/Face Mask Dataset/Train'\n",
    "categories=os.listdir(data_path)\n",
    "labels=[i for i in range(len(categories))]\n",
    "\n",
    "label_dict=dict(zip(categories,labels)) #empty dictionary\n",
    "\n",
    "print(label_dict)\n",
    "print(categories)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=100\n",
    "data=[]\n",
    "target=[]\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    folder_path=os.path.join(data_path,category)\n",
    "    img_names=os.listdir(folder_path)\n",
    "        \n",
    "    for img_name in img_names:\n",
    "        img_path=os.path.join(folder_path,img_name)\n",
    "        img=cv2.imread(img_path)\n",
    "\n",
    "        try:\n",
    "            gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)           \n",
    "            #Coverting the image into gray scale\n",
    "            resized=cv2.resize(gray,(img_size,img_size))\n",
    "            #resizing the gray scale into 50x50, since we need a fixed common size for all the images in the dataset\n",
    "            data.append(resized)\n",
    "            target.append(label_dict[category])\n",
    "            #appending the image and the label(categorized) into the list (dataset)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Exception:',e)\n",
    "            #if any exception rasied, the exception will be printed here. And pass to the next image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "data=np.array(data)/255.0\n",
    "data=np.reshape(data,(data.shape[0],img_size,img_size,1))\n",
    "target=np.array(target)\n",
    "new_target=np_utils.to_categorical(target)\n",
    "\n",
    "np.save('data',data)\n",
    "np.save('target',new_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data=np.load('/home/suhruth/Downloads/data.npy')\n",
    "target=np.load('/home/suhruth/Downloads/target.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(200,(3,3),input_shape=(100,100,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#The first CNN layer followed by Relu and MaxPooling layers\n",
    "\n",
    "model.add(Conv2D(100,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#The second convolution layer followed by Relu and MaxPooling layers\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "#Flatten layer to stack the output convolutions from second convolution layer\n",
    "model.add(Dense(50,activation='relu'))\n",
    "#Dense layer of 64 neurons\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "#The Final layer with two outputs for two categories\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data,test_data,train_target,test_target=train_test_split(data,target,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "31/31 [==============================] - 34s 1s/step - loss: 0.7745 - accuracy: 0.5133 - val_loss: 0.6643 - val_accuracy: 0.5524\n",
      "INFO:tensorflow:Assets written to: model-001.model/assets\n",
      "Epoch 2/20\n",
      "31/31 [==============================] - 35s 1s/step - loss: 0.5752 - accuracy: 0.7207 - val_loss: 0.4947 - val_accuracy: 0.7782\n",
      "INFO:tensorflow:Assets written to: model-002.model/assets\n",
      "Epoch 3/20\n",
      "31/31 [==============================] - 33s 1s/step - loss: 0.4009 - accuracy: 0.8094 - val_loss: 0.4615 - val_accuracy: 0.7944\n",
      "INFO:tensorflow:Assets written to: model-003.model/assets\n",
      "Epoch 4/20\n",
      "31/31 [==============================] - 31s 1s/step - loss: 0.3256 - accuracy: 0.8692 - val_loss: 0.3191 - val_accuracy: 0.8589\n",
      "INFO:tensorflow:Assets written to: model-004.model/assets\n",
      "Epoch 5/20\n",
      "31/31 [==============================] - 31s 1s/step - loss: 0.1911 - accuracy: 0.9226 - val_loss: 0.2760 - val_accuracy: 0.8992\n",
      "INFO:tensorflow:Assets written to: model-005.model/assets\n",
      "Epoch 6/20\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.1341 - accuracy: 0.9557 - val_loss: 0.3115 - val_accuracy: 0.8669\n",
      "Epoch 7/20\n",
      "31/31 [==============================] - 31s 1s/step - loss: 0.1495 - accuracy: 0.9410 - val_loss: 0.2175 - val_accuracy: 0.9032\n",
      "INFO:tensorflow:Assets written to: model-007.model/assets\n",
      "Epoch 8/20\n",
      "31/31 [==============================] - 31s 1s/step - loss: 0.1470 - accuracy: 0.9397 - val_loss: 0.1905 - val_accuracy: 0.9315\n",
      "INFO:tensorflow:Assets written to: model-008.model/assets\n",
      "Epoch 9/20\n",
      "31/31 [==============================] - 31s 1s/step - loss: 0.1211 - accuracy: 0.9662 - val_loss: 0.1194 - val_accuracy: 0.9516\n",
      "INFO:tensorflow:Assets written to: model-009.model/assets\n",
      "Epoch 10/20\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.0605 - accuracy: 0.9792 - val_loss: 0.1364 - val_accuracy: 0.9476\n",
      "Epoch 11/20\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.0626 - accuracy: 0.9747 - val_loss: 0.1332 - val_accuracy: 0.9355\n",
      "Epoch 12/20\n",
      "31/31 [==============================] - 32s 1s/step - loss: 0.0273 - accuracy: 0.9943 - val_loss: 0.1203 - val_accuracy: 0.9597\n",
      "Epoch 13/20\n",
      "31/31 [==============================] - 31s 1s/step - loss: 0.0293 - accuracy: 0.9936 - val_loss: 0.1149 - val_accuracy: 0.9677\n",
      "INFO:tensorflow:Assets written to: model-013.model/assets\n",
      "Epoch 14/20\n",
      "31/31 [==============================] - 31s 1s/step - loss: 0.0270 - accuracy: 0.9915 - val_loss: 0.1221 - val_accuracy: 0.9556\n",
      "Epoch 15/20\n",
      "31/31 [==============================] - 31s 1s/step - loss: 0.0244 - accuracy: 0.9943 - val_loss: 0.1103 - val_accuracy: 0.9556\n",
      "INFO:tensorflow:Assets written to: model-015.model/assets\n",
      "Epoch 16/20\n",
      "31/31 [==============================] - 36s 1s/step - loss: 0.0346 - accuracy: 0.9829 - val_loss: 0.1049 - val_accuracy: 0.9677\n",
      "INFO:tensorflow:Assets written to: model-016.model/assets\n",
      "Epoch 17/20\n",
      "31/31 [==============================] - 38s 1s/step - loss: 0.0322 - accuracy: 0.9868 - val_loss: 0.2183 - val_accuracy: 0.9153\n",
      "Epoch 18/20\n",
      "31/31 [==============================] - 33s 1s/step - loss: 0.0432 - accuracy: 0.9854 - val_loss: 0.0940 - val_accuracy: 0.9677\n",
      "INFO:tensorflow:Assets written to: model-018.model/assets\n",
      "Epoch 19/20\n",
      "31/31 [==============================] - 50s 2s/step - loss: 0.0375 - accuracy: 0.9879 - val_loss: 0.1571 - val_accuracy: 0.9395\n",
      "Epoch 20/20\n",
      "31/31 [==============================] - 39s 1s/step - loss: 0.0260 - accuracy: 0.9872 - val_loss: 0.1210 - val_accuracy: 0.9556\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')\n",
    "history=model.fit(train_data,train_target,epochs=20,callbacks=[checkpoint],validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_config = '/home/suhruth/Documents/yolove3cfg.txt'\n",
    "yolo_weights = '/home/suhruth/Downloads/yolov3.weights'\n",
    "classes_file = '/home/suhruth/Documents/coco.names'\n",
    "\n",
    "with open(classes_file,'r') as f:\n",
    "    classes=[line.strip() for line in f.readlines()]\n",
    "   \n",
    "net=cv2.dnn.readNet(yolo_weights,yolo_config)\n",
    "\n",
    "def humanDetect(image):\n",
    "   \n",
    "    blob=cv2.dnn.blobFromImage(image,1/255,(512,512),(0,0,0),True,crop=False)\n",
    "    \n",
    "    net.setInput(blob)\n",
    "    layer_names=net.getLayerNames()\n",
    "    output_layers=[layer_names[i[0]-1] for i in net.getUnconnectedOutLayers()]\n",
    "    outs=net.forward(output_layers)\n",
    "    class_ids= list()\n",
    "    confidences= list()\n",
    "    boxes= list()\n",
    "\n",
    "    for out in outs:\n",
    "        # iterate over anchor boxes for human class\n",
    "        for detection in out:\n",
    "            #bounding box\n",
    "            center_x=int(detection[0] * image.shape[1])\n",
    "            center_y=int(detection[1] * image.shape[0])\n",
    "            w=int(detection[2] * image.shape[1])\n",
    "            h=int(detection[3] * image.shape[0])\n",
    "            x=center_x - w // 2\n",
    "            y=center_y - h // 2\n",
    "            boxes.append([x,y,w,h])\n",
    "            \n",
    "            #class\n",
    "            class_id=np.argmax(detection[5:])\n",
    "            class_ids.append(class_id)\n",
    "            confidence=detection[4]\n",
    "            confidences.append(float(confidence))\n",
    "   \n",
    "    #non-max supression\n",
    "    ids=cv2.dnn.NMSBoxes(boxes, confidences, score_threshold=0.5, nms_threshold=0.3)\n",
    "    #draw the bounding boxes on the image\n",
    "    colors= np.random.uniform(0,255,size=(len(classes),3))\n",
    "\n",
    "    imgs = []\n",
    "\n",
    "    for i in ids:\n",
    "        i=i[0]\n",
    "        x,y,w,h=boxes[i]\n",
    "        class_id=class_ids[i]\n",
    "        color=colors[class_id]\n",
    "        if classes[class_id]=='person':\n",
    "            crop= image[y:y+h,x:x+w]\n",
    "            imgs.append([crop, [x, y, w, h]])           \n",
    "            # return [crop, [x, y, w, h]]\n",
    "\n",
    "    if (len(imgs) == 0):\n",
    "      return None\n",
    "    else: \n",
    "      return imgs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.97772425 0.02227579]] 0\n",
      "[[0.9779874  0.02201259]] 0\n",
      "[[0.9748669  0.02513304]] 0\n",
      "[[0.9796652 0.0203348]] 0\n",
      "[[0.961725   0.03827501]] 0\n",
      "[[0.9716684  0.02833154]] 0\n",
      "[[0.97397715 0.02602284]] 0\n",
      "[[0.98449695 0.01550303]] 0\n",
      "[[0.9675437  0.03245626]] 0\n",
      "[[0.9613936  0.03860636]] 0\n",
      "[[0.9416248  0.05837516]] 0\n",
      "[[0.9632315  0.03676846]] 0\n",
      "[[0.9648855  0.03511459]] 0\n",
      "[[0.97121847 0.02878156]] 0\n",
      "[[0.97396106 0.02603896]] 0\n",
      "[[0.9785065  0.02149354]] 0\n",
      "[[0.97830063 0.02169937]] 0\n",
      "[[0.97067744 0.02932261]] 0\n",
      "[[0.9852409  0.01475908]] 0\n",
      "[[0.96846414 0.03153589]] 0\n",
      "[[0.97558993 0.02441003]] 0\n",
      "[[0.98164475 0.01835524]] 0\n",
      "[[0.95954037 0.0404596 ]] 0\n",
      "[[0.9692077  0.03079233]] 0\n",
      "[[0.98099077 0.0190092 ]] 0\n",
      "[[0.9800426  0.01995741]] 0\n",
      "[[0.97995114 0.02004893]] 0\n",
      "[[0.9737882  0.02621183]] 0\n",
      "[[0.9737254  0.02627463]] 0\n",
      "[[0.9739953  0.02600465]] 0\n",
      "[[0.96762305 0.03237693]] 0\n",
      "[[0.97623956 0.02376045]] 0\n",
      "[[0.9753695  0.02463044]] 0\n",
      "[[0.9690606  0.03093944]] 0\n",
      "[[0.9678821  0.03211793]] 0\n",
      "[[0.97123533 0.02876469]] 0\n",
      "[[0.96307063 0.03692935]] 0\n",
      "[[0.97688156 0.02311845]] 0\n",
      "[[0.968979   0.03102104]] 0\n",
      "[[0.98227733 0.01772268]] 0\n",
      "[[0.96631473 0.03368526]] 0\n",
      "[[0.9788184  0.02118159]] 0\n",
      "[[0.9776267  0.02237328]] 0\n",
      "[[0.9759847  0.02401531]] 0\n",
      "[[0.9710435  0.02895644]] 0\n",
      "[[0.97559196 0.02440807]] 0\n",
      "[[0.97242016 0.02757978]] 0\n",
      "[[0.97753125 0.02246877]] 0\n",
      "[[0.97664624 0.02335376]] 0\n",
      "[[0.97735107 0.02264892]] 0\n",
      "[[0.96651214 0.03348788]] 0\n",
      "[[0.9765627 0.0234373]] 0\n",
      "[[0.94374704 0.05625293]] 0\n",
      "[[0.9302084  0.06979162]] 0\n",
      "[[0.8658307 0.1341693]] 0\n",
      "[[0.01629035 0.9837097 ]] 1\n",
      "[[0.03073255 0.9692675 ]] 1\n",
      "[[0.03397234 0.9660276 ]] 1\n",
      "[[0.02481012 0.9751899 ]] 1\n",
      "[[0.03514342 0.9648566 ]] 1\n",
      "[[0.04740218 0.95259786]] 1\n",
      "[[0.02753178 0.9724682 ]] 1\n",
      "[[0.02842282 0.97157717]] 1\n",
      "[[0.01950868 0.9804913 ]] 1\n",
      "[[0.0169083  0.98309165]] 1\n",
      "[[0.01447743 0.9855225 ]] 1\n",
      "[[0.02606566 0.9739343 ]] 1\n",
      "[[0.02608425 0.97391576]] 1\n",
      "[[0.02048179 0.97951823]] 1\n",
      "[[0.02922828 0.9707717 ]] 1\n",
      "[[0.04528689 0.95471317]] 1\n",
      "[[0.0967001  0.90329987]] 1\n",
      "[[0.07391024 0.92608976]] 1\n",
      "[[0.04472325 0.9552767 ]] 1\n",
      "[[0.06427907 0.9357209 ]] 1\n",
      "[[0.05350814 0.9464919 ]] 1\n",
      "[[0.04071867 0.9592813 ]] 1\n",
      "[[0.04201124 0.9579887 ]] 1\n",
      "[[0.05667163 0.9433284 ]] 1\n",
      "[[0.05901589 0.94098413]] 1\n",
      "[[0.04909644 0.9509036 ]] 1\n",
      "[[0.04097736 0.9590226 ]] 1\n",
      "[[0.05200901 0.94799095]] 1\n",
      "[[0.04381825 0.9561817 ]] 1\n",
      "[[0.03758891 0.9624111 ]] 1\n",
      "[[0.02528117 0.97471887]] 1\n",
      "[[0.01896758 0.98103243]] 1\n",
      "[[0.03368321 0.9663168 ]] 1\n",
      "[[0.02583029 0.9741698 ]] 1\n",
      "[[0.0293572  0.97064286]] 1\n",
      "[[0.04084789 0.9591521 ]] 1\n",
      "[[0.0512872 0.9487128]] 1\n",
      "[[0.02816209 0.971838  ]] 1\n",
      "[[0.02694165 0.9730584 ]] 1\n",
      "[[0.03492299 0.96507704]] 1\n",
      "[[0.03543684 0.96456313]] 1\n",
      "[[0.02375902 0.97624093]] 1\n",
      "[[0.02350819 0.97649187]] 1\n",
      "[[0.02244687 0.9775531 ]] 1\n",
      "[[0.02395262 0.9760474 ]] 1\n",
      "[[0.02839016 0.97160983]] 1\n",
      "[[0.03146901 0.968531  ]] 1\n",
      "[[0.02941007 0.97058994]] 1\n",
      "[[0.03522542 0.9647746 ]] 1\n",
      "[[0.02505458 0.9749454 ]] 1\n",
      "[[0.02965822 0.9703418 ]] 1\n",
      "[[0.03711582 0.9628841 ]] 1\n",
      "[[0.03344943 0.9665505 ]] 1\n",
      "[[0.03276702 0.967233  ]] 1\n",
      "[[0.03915523 0.96084476]] 1\n",
      "[[0.05719091 0.9428091 ]] 1\n",
      "[[0.03278548 0.9672145 ]] 1\n",
      "[[0.04884157 0.95115846]] 1\n",
      "[[0.05255532 0.9474447 ]] 1\n",
      "[[0.0661507  0.93384933]] 1\n",
      "[[0.06582741 0.9341726 ]] 1\n",
      "[[0.08042563 0.91957444]] 1\n",
      "[[0.06195245 0.9380475 ]] 1\n",
      "[[0.08089161 0.91910845]] 1\n",
      "[[0.06785113 0.9321489 ]] 1\n",
      "[[0.07042652 0.92957354]] 1\n",
      "[[0.05988169 0.9401183 ]] 1\n",
      "[[0.0844044 0.9155956]] 1\n",
      "[[0.0560284 0.9439716]] 1\n",
      "[[0.06183515 0.9381648 ]] 1\n",
      "[[0.07774242 0.9222576 ]] 1\n",
      "[[0.0530523 0.9469477]] 1\n",
      "[[0.06980374 0.9301963 ]] 1\n",
      "[[0.03803926 0.9619607 ]] 1\n",
      "[[0.03586333 0.96413666]] 1\n",
      "[[0.02756598 0.97243404]] 1\n",
      "[[0.05955281 0.94044715]] 1\n",
      "[[0.04615211 0.95384794]] 1\n",
      "[[0.95075756 0.0492425 ]] 0\n",
      "[[0.9056346  0.09436537]] 0\n",
      "[[0.9445128  0.05548718]] 0\n",
      "[[0.93433225 0.06566771]] 0\n",
      "[[0.92664903 0.07335094]] 0\n",
      "[[0.92658544 0.07341449]] 0\n",
      "[[0.9410289  0.05897112]] 0\n",
      "[[0.91409266 0.08590732]] 0\n",
      "[[0.88888985 0.1111102 ]] 0\n",
      "[[0.9173116  0.08268841]] 0\n",
      "[[0.9045653  0.09543472]] 0\n",
      "[[0.86615425 0.13384575]] 0\n",
      "[[0.921508   0.07849199]] 0\n",
      "[[0.9019682  0.09803182]] 0\n",
      "[[0.88032407 0.11967596]] 0\n",
      "[[0.83447397 0.16552597]] 0\n",
      "[[0.76849425 0.23150572]] 0\n",
      "[[0.83459526 0.16540469]] 0\n",
      "[[0.74189645 0.25810355]] 0\n",
      "[[0.7995011  0.20049885]] 0\n",
      "[[0.8166198 0.1833802]] 0\n",
      "[[0.8197708  0.18022919]] 0\n",
      "[[0.84700423 0.15299581]] 0\n",
      "[[0.8749229  0.12507714]] 0\n",
      "[[0.891738   0.10826199]] 0\n",
      "[[0.90480614 0.09519383]] 0\n",
      "[[0.93244034 0.06755962]] 0\n",
      "[[0.9377218  0.06227814]] 0\n",
      "[[0.9427391  0.05726081]] 0\n",
      "[[0.9308788  0.06912122]] 0\n",
      "[[0.9044962  0.09550382]] 0\n",
      "[[0.8903301  0.10966992]] 0\n",
      "[[0.90618294 0.09381709]] 0\n",
      "[[0.92398596 0.076014  ]] 0\n",
      "[[0.8797623  0.12023776]] 0\n",
      "[[0.8861723  0.11382774]] 0\n",
      "[[0.9215419  0.07845805]] 0\n",
      "[[0.91511613 0.08488385]] 0\n",
      "[[0.9108778  0.08912219]] 0\n",
      "[[0.9231663  0.07683375]] 0\n",
      "[[0.9187063  0.08129372]] 0\n",
      "[[0.92454624 0.07545369]] 0\n",
      "[[0.9357355  0.06426445]] 0\n",
      "[[0.9110903  0.08890969]] 0\n",
      "[[0.8950957  0.10490431]] 0\n",
      "[[0.9169852  0.08301485]] 0\n",
      "[[0.91477853 0.08522141]] 0\n",
      "[[0.9400434  0.05995664]] 0\n",
      "[[0.9174014 0.0825986]] 0\n",
      "[[0.9194719  0.08052807]] 0\n",
      "[[0.91011566 0.08988432]] 0\n",
      "[[0.9233375 0.0766625]] 0\n",
      "[[0.82435447 0.17564556]] 0\n",
      "[[0.90896827 0.09103172]] 0\n",
      "[[0.94061685 0.05938321]] 0\n",
      "[[0.9006196  0.09938038]] 0\n",
      "[[0.91673136 0.08326864]] 0\n",
      "[[0.9223794  0.07762059]] 0\n",
      "[[0.8875225 0.1124775]] 0\n",
      "[[0.87586004 0.12413996]] 0\n",
      "[[0.9411903  0.05880972]] 0\n",
      "[[0.95305187 0.04694818]] 0\n",
      "[[0.90279776 0.09720225]] 0\n",
      "[[0.9534104  0.04658962]] 0\n",
      "[[0.9529771  0.04702289]] 0\n",
      "[[0.9579754 0.0420246]] 0\n",
      "[[0.96327686 0.03672312]] 0\n",
      "[[0.9514842  0.04851581]] 0\n",
      "[[0.94176364 0.05823635]] 0\n",
      "[[0.9360455  0.06395446]] 0\n",
      "[[0.9270519  0.07294811]] 0\n",
      "[[0.9336822  0.06631784]] 0\n",
      "[[0.9718736  0.02812645]] 0\n",
      "[[0.92885435 0.07114571]] 0\n"
     ]
    }
   ],
   "source": [
    "model = load_model('/home/suhruth/model-009.model')\n",
    "\n",
    "face_clsfr=cv2.CascadeClassifier('/home/suhruth/Documents/haarcascade_frontalface_alt.xml')\n",
    "cap=cv2.VideoCapture(0)\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "out = cv2.VideoWriter('/home/suhruth/Documents/finals.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 20, (frame_width,frame_height))\n",
    "labels_dict={0:'MASK',1:'NO MASK'}\n",
    "color_dict={0:(0,255,0),1:(0,0,255)}\n",
    "frame_index=0\n",
    "while(True):\n",
    "    ret,img=cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_clsfr.detectMultiScale(gray,1.3,5)  \n",
    "    for (x,y,w,h) in faces:\n",
    "        face_img=gray[y:y+w,x:x+w]\n",
    "        resized=cv2.resize(face_img,(100,100))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,100,100,1))\n",
    "        result=model.predict(reshaped)\n",
    "        label=result[0][1]\n",
    "        if label >=0.5:\n",
    "            label = 1\n",
    "        else: label = 0\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(img,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        print(result,label)\n",
    "        cv2.putText(img, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "    out.write(img)\n",
    "    cv2.imshow('LIVE',img)\n",
    "    key=cv2.waitKey(1)\n",
    "    if(key==27):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
